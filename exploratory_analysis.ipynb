{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing original dataset via CSV (https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset)\n",
    "import pandas as pd\n",
    "def load_dataset(csv):\n",
    "  df = pd.read_csv('online_shoppers_intention.csv')\n",
    "  return df\n",
    "csv = 'online_shoppers_intention.csv'\n",
    "df = load_dataset(csv)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic for Exploratory Report\n",
    "from ydata_profiling import ProfileReport\n",
    "profile = ProfileReport(df, title='Exploratory Analysis Report')\n",
    "profile.to_file('exploratory_analysis_report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "#Converting Categorical into Integers and Use One-Hot Coding\n",
    "sessions = df\n",
    "sessions['Revenue'] = sessions['Revenue'].astype(int)#converting revenue into 0 and 1\n",
    "sessions['SpecialDay'] = sessions['SpecialDay'].astype(int)\n",
    "# One-hot encode VisitorType\n",
    "visitor_type_dummies = pd.get_dummies(sessions['VisitorType'], prefix='VisitorType').astype(int)\n",
    "sessions = pd.concat([sessions, visitor_type_dummies], axis=1)\n",
    "sessions.drop('VisitorType', axis=1, inplace=True)\n",
    "sessions['Weekend']=sessions['Weekend'].astype(int)\n",
    "sessions['Month'] = sessions['Month'].map({'Feb':2, 'Mar':3,'May':5, 'June':6, 'Jul':7, 'Aug':8, 'Sep':9, 'Oct':10, 'Nov':11, 'Dec':12})\n",
    "\n",
    "sessions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Relevant Columns\n",
    "sessions['SessionDuration']=sessions['Administrative_Duration']+sessions['Informational_Duration']+sessions['ProductRelated_Duration']\n",
    "sessions['TotalPagesViewed']=sessions['Administrative']+sessions['Informational']+sessions['ProductRelated']\n",
    "sessions['BounceRates'] = sessions['BounceRates'].astype(float)\n",
    "\n",
    "sessions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions['VisitorType'].unique()\n",
    "#print(sessions.head())\n",
    "#print(sessions[(sessions['VisitorType'] == 'Other') & (sessions['Revenue'] == 1)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sessions with Revenue based on Pages Viewed \n",
    "values_with_Rev = df[df['Revenue']==1].shape[0]\n",
    "print(\"Total Values in dataset with Rev\", values_with_Rev)\n",
    "\n",
    "values_with_Rev_Admin = df[(df['Revenue']==1) & (df['Administrative']>0)].shape[0]\n",
    "print(\"Values with Rev and atleast one Administrative Page Viewed\", values_with_Rev_Admin)\n",
    "\n",
    "values_with_Rev_Product = df[(df['Revenue']==1) & (df['ProductRelated']>0)].shape[0]\n",
    "print(\"Values with Rev and atleast one Product page viewed\", values_with_Rev_Product)\n",
    "\n",
    "values_with_Rev_Informational = df[(df['Revenue']==1) & (df['Informational']>0)].shape[0]\n",
    "print(\"Values with Rev and atleast one Informational page viewed\", values_with_Rev_Informational)\n",
    "\n",
    "values_with_Rev_GreaterThanAvgAdmin = df[(df['Revenue']==1) & (df['Administrative_Duration']>df['Administrative_Duration'].mean())].shape[0]\n",
    "print(\"Values with Rev and Administrative Duration over the mean Duration\", values_with_Rev_GreaterThanAvgAdmin)\n",
    "\n",
    "values_with_Rev_GreaterThanAvgProduct = df[(df['Revenue']==1) & (df['ProductRelated_Duration']>df['ProductRelated_Duration'].mean())].shape[0]\n",
    "print(\"Values with Rev and ProductRelated_Duration  over the mean Duration\", values_with_Rev_GreaterThanAvgProduct)\n",
    "\n",
    "values_with_Rev_GreaterThanAvgInformational = df[(df['Revenue']==1) & (df['Informational_Duration']>df['Informational_Duration'].mean())].shape[0]\n",
    "print(\"Values with Rev and Informational Duration over the mean Duration\", values_with_Rev_GreaterThanAvgInformational)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Months with Revenue vs Months without Revenue\n",
    "values_with_Rev_Feb = df[(df['Revenue']==1) & (df['Month']=='Feb')].shape[0]\n",
    "print(\"Values with Rev and Feb\", values_with_Rev_Feb)\n",
    "values_with_Rev_Feb = df[(df['Revenue']==0) & (df['Month']=='Feb')].shape[0]\n",
    "print(\"Values with Rev and Feb\", values_with_Rev_Feb)\n",
    "\n",
    "values_with_Rev_March = df[(df['Revenue']==1) & (df['Month']=='Mar')].shape[0]\n",
    "print(\"Values with Rev and March\", values_with_Rev_March)\n",
    "values_with_Rev_March = df[(df['Revenue']==0) & (df['Month']=='Mar')].shape[0]\n",
    "print(\"Values with Rev and March\", values_with_Rev_March)\n",
    "\n",
    "values_with_Rev_May = df[(df['Revenue']==1) & (df['Month']=='May')].shape[0]\n",
    "print(\"Values with Rev and May\", values_with_Rev_May)\n",
    "values_with_Rev_May = df[(df['Revenue']==0) & (df['Month']=='May')].shape[0]\n",
    "print(\"Values with Rev and May\", values_with_Rev_May)\n",
    "\n",
    "values_with_Rev_June = df[(df['Revenue']==1) & (df['Month']=='June')].shape[0]\n",
    "print(\"Values with Rev and June\", values_with_Rev_June)\n",
    "values_with_Rev_June = df[(df['Revenue']==0) & (df['Month']=='June')].shape[0]\n",
    "print(\"Values with Rev and June\", values_with_Rev_June)\n",
    "\n",
    "values_with_Rev_July = df[(df['Revenue']==1) & (df['Month']=='July')].shape[0]\n",
    "print(\"Values with Rev and July\", values_with_Rev_July)\n",
    "values_with_Rev_July = df[(df['Revenue']==0) & (df['Month']=='July')].shape[0]\n",
    "print(\"Values with Rev and July\", values_with_Rev_July)\n",
    "\n",
    "values_with_Rev_Aug = df[(df['Revenue']==1) & (df['Month']=='Aug')].shape[0]\n",
    "print(\"Values with Rev and August\", values_with_Rev_Aug)\n",
    "values_with_Rev_Aug = df[(df['Revenue']==0) & (df['Month']=='Aug')].shape[0]\n",
    "print(\"Values with Rev and August\", values_with_Rev_Aug)\n",
    "\n",
    "values_with_Rev_Sep = df[(df['Revenue']==1) & (df['Month']=='Sep')].shape[0]\n",
    "print(\"Values with Rev and Sep\", values_with_Rev_Sep)\n",
    "values_with_Rev_Sep = df[(df['Revenue']==0) & (df['Month']=='Sep')].shape[0]\n",
    "print(\"Values with Rev and Sep\", values_with_Rev_Sep)\n",
    "\n",
    "values_with_Rev_Oct = df[(df['Revenue']==1) & (df['Month']=='Oct')].shape[0]\n",
    "print(\"Values with Rev and Oct\", values_with_Rev_Oct)\n",
    "values_with_Rev_Oct = df[(df['Revenue']==0) & (df['Month']=='Oct')].shape[0]\n",
    "print(\"Values with Rev and Oct\", values_with_Rev_Oct)\n",
    "\n",
    "values_with_Rev_Nov = df[(df['Revenue']==1) & (df['Month']=='Nov')].shape[0]\n",
    "print(\"Values with Rev and Nov\", values_with_Rev_Nov)\n",
    "values_with_Rev_Nov = df[(df['Revenue']==0) & (df['Month']=='Nov')].shape[0]\n",
    "print(\"Values with Rev and Nov\", values_with_Rev_Nov)\n",
    "\n",
    "values_with_Rev_Dec = df[(df['Revenue']==1) & (df['Month']=='Dec')].shape[0]\n",
    "print(\"Values with Rev and Dec\", values_with_Rev_Dec)\n",
    "values_with_Rev_Dec = df[(df['Revenue']==0) & (df['Month']=='Dec')].shape[0]\n",
    "print(\"Values with Rev and Dec\", values_with_Rev_Dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revenue based on Different TrafficTypes [1-20] \n",
    "traffic_types = df['TrafficType'].unique()\n",
    "\n",
    "# Iterate over each TrafficType and calculate the values with Revenue\n",
    "for traffic_type in traffic_types:\n",
    "    values_with_Rev = df[(df['Revenue'] == 1) & (df['TrafficType'] == traffic_type)].shape[0]\n",
    "    print(f\"Values with Rev and TrafficType {traffic_type}: {values_with_Rev}\")\n",
    "    \n",
    "    values_without_Rev = df[(df['Revenue'] == 0) & (df['TrafficType'] == traffic_type)].shape[0]\n",
    "    print(f\"Values without Rev and TrafficType {traffic_type}: {values_without_Rev}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revenue based on Different VisitorTypes[Returning, New, Other]\n",
    "visitor_types = df['VisitorType'].unique()\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over each VisitorType and calculate the values with and without Revenue\n",
    "for visitor_type in visitor_types:\n",
    "    values_with_Rev = df[(df['Revenue'] == 1) & (df['VisitorType'] == visitor_type)].shape[0]\n",
    "    values_without_Rev = df[(df['Revenue'] == 0) & (df['VisitorType'] == visitor_type)].shape[0]\n",
    "    \n",
    "    results.append({\n",
    "        'VisitorType': visitor_type,\n",
    "        'Values with Revenue': values_with_Rev,\n",
    "        'Values without Revenue': values_without_Rev\n",
    "    })\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revenue based on Operating System [1-8]\n",
    "#OperatingSystems\n",
    "# Get unique VisitorTypes\n",
    "operating_systems = df['OperatingSystems'].unique()\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over each VisitorType and calculate the values with and without Revenue\n",
    "for operating_system in operating_systems:\n",
    "    values_with_Rev = df[(df['Revenue'] == 1) & (df['OperatingSystems'] == operating_system)].shape[0]\n",
    "    values_without_Rev = df[(df['Revenue'] == 0) & (df['OperatingSystems'] == operating_system)].shape[0]\n",
    "    \n",
    "    results.append({\n",
    "        'operating_systems': operating_system,\n",
    "        'Values with Revenue': values_with_Rev,\n",
    "        'Values without Revenue': values_without_Rev\n",
    "    })\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(type(results))\n",
    "print(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revenue based on Weekend\n",
    "# Get unique VisitorTypes\n",
    "weekend = df['Weekend'].unique()\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "# Iterate over each VisitorType and calculate the values with and without Revenue\n",
    "for weekend_type in weekend:\n",
    "    values_with_Rev = df[(df['Revenue'] == 1) & (df['Weekend'] == weekend_type)].shape[0]\n",
    "    values_without_Rev = df[(df['Revenue'] == 0) & (df['Weekend'] == weekend_type)].shape[0]\n",
    "    \n",
    "    results.append({\n",
    "        'weekend_type': weekend_type,\n",
    "        'Values with Revenue': values_with_Rev,\n",
    "        'Values without Revenue': values_without_Rev\n",
    "    })\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(type(results))\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revenue based on Browser [1-13]\n",
    "# Get unique VisitorTypes\n",
    "browsers = df['Browser'].unique()\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over each VisitorType and calculate the values with and without Revenue\n",
    "for browser_type in browsers:\n",
    "    values_with_Rev = df[(df['Revenue'] == 1) & (df['Browser'] == browser_type)].shape[0]\n",
    "    values_without_Rev = df[(df['Revenue'] == 0) & (df['Browser'] == browser_type)].shape[0]\n",
    "    \n",
    "    results.append({\n",
    "        'Browser Type': browser_type,\n",
    "        'Values with Revenue': values_with_Rev,\n",
    "        'Values without Revenue': values_without_Rev\n",
    "    })\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(type(results))\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chi Square Test for Revenue vs Months (Long-way)\n",
    "# Observed values for revenue and no revenue across months\n",
    "observed = [\n",
    "    [3, 192, 365, 29, 0, 76, 86, 115, 760, 216],   # Revenue = 1\n",
    "    [181, 1715, 2999, 259, 0, 357, 362, 434, 2238, 1511]  # Revenue = 0\n",
    "]\n",
    "\n",
    "# Add a small constant to avoid zero elements\n",
    "observed = [[value + 0.5 for value in row] for row in observed]\n",
    "\n",
    "# Convert to a DataFrame for visual clarity (optional)\n",
    "months = ['Feb', 'Mar', 'May', 'June', 'July', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "observed_df = pd.DataFrame(observed, index=['Revenue', 'No Revenue'], columns=months)\n",
    "print(\"Observed Frequencies:\\n\", observed_df)\n",
    "\n",
    "# Perform chi-square test\n",
    "chi2, p_value, dof, expected = chi2_contingency(observed)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nChi-square Statistic:\", chi2)\n",
    "print(\"p-value:\", p_value)\n",
    "print(\"Degrees of Freedom:\", dof)\n",
    "print(\"\\nExpected Frequencies:\\n\", pd.DataFrame(expected, index=['Revenue', 'No Revenue'], columns=months))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chi Square- ['Month', 'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType', 'Weekend']\n",
    "# Convert Revenue to categorical (1 for revenue, 0 for no revenue)\n",
    "df['Revenue'] = df['Revenue'].astype(int)\n",
    "\n",
    "# Define categorical variables to test against Revenue\n",
    "categorical_vars = ['Month', 'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType', 'Weekend']\n",
    "\n",
    "# Running chi-square tests for each categorical variable against Revenue\n",
    "results = {}\n",
    "\n",
    "for var in categorical_vars:\n",
    "    contingency_table = pd.crosstab(df[var], df['Revenue'])\n",
    "    \n",
    "    # Perform chi-square test\n",
    "    chi2, p, dof, ex = chi2_contingency(contingency_table)\n",
    "    \n",
    "    results[var] = {\n",
    "        'Chi-square Statistic': chi2,\n",
    "        'p-value': p,\n",
    "        'Degrees of Freedom': dof\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "for var, result in results.items():\n",
    "    print(f\"{var}:\\nChi-square Statistic: {result['Chi-square Statistic']}, p-value: {result['p-value']}, Degrees of Freedom: {result['Degrees of Freedom']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph Sessions Duration for Rev vs non-Rev\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sessions.boxplot(column='SessionDuration', by='Revenue')\n",
    "plt.xlabel('Rev')\n",
    "plt.ylabel('Session Duration')\n",
    "plt.title('Session Duration vs Revenue')\n",
    "plt.suptitle('')  # Suppress the automatic title to keep the plot clean\n",
    "plt.show()\n",
    "sessions['SessionDuration_Binned'] = pd.cut(sessions['SessionDuration'], bins=4, labels=['Very Short', 'Short', 'Medium', 'Long'])\n",
    "sessions.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data - replace with your actual session_duration data\n",
    "np.random.seed(0)\n",
    "sessions['SessionDuration']\n",
    "# Standardize\n",
    "standardized_duration = (sessions['SessionDuration'] - sessions['SessionDuration'].mean()) / sessions['SessionDuration'].std()\n",
    "\n",
    "# Log transform and standardize\n",
    "log_standardized_duration = (np.log1p(sessions['SessionDuration']) - np.log1p(sessions['SessionDuration']).mean()) / np.log1p(sessions['SessionDuration']).std()\n",
    "\n",
    "# Plot the distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.kdeplot(standardized_duration, label='Standardized', color='blue')\n",
    "sns.kdeplot(log_standardized_duration, label='Log-Standardized', color='red')\n",
    "\n",
    "plt.title(\"Comparison of Standardized vs. Log-Standardized Session Duration\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming session_duration_summary is a pandas Series\n",
    "session_duration_summary = pd.Series(sessions['SessionDuration'])\n",
    "\n",
    "# Calculate summary statistics\n",
    "mean = session_duration_summary.mean()\n",
    "median = session_duration_summary.median()\n",
    "std_dev = session_duration_summary.std()\n",
    "skewness = skew(session_duration_summary)\n",
    "kurt = kurtosis(session_duration_summary)\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Median: {median}\")\n",
    "print(f\"Standard Deviation: {std_dev}\")\n",
    "print(f\"Skewness: {skewness}\")\n",
    "print(f\"Kurtosis: {kurt}\")\n",
    "\n",
    "sessions['Log_ProductRelated_Duration'] = np.log1p(sessions['ProductRelated_Duration'])  # log(1+x)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(df['Log_ProductRelated_Duration'], shade=True)\n",
    "plt.title('Kernel Density Estimate of Log Product Duration ')\n",
    "plt.xlabel('Standardized Log Product Duration')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "# Standardize the data\n",
    "log_standardized_data = (sessions['Log_ProductRelated_Duration'] - sessions['Log_ProductRelated_Duration'].mean()) / sessions['Log_ProductRelated_Duration'].std()\n",
    "\n",
    "# Plot the KDE\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(log_standardized_data, shade=True)\n",
    "plt.title('Kernel Density Estimate of Log and Standardized Session Duration')\n",
    "plt.xlabel('Standardized Session Duration')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log and Standardizing Product Duration (Unfinished)\n",
    "df['productRelatedDuration_Standardized'] = (df['ProductRelated_Duration'] - df['ProductRelated_Duration'].mean()) / df['ProductRelated_Duration'].std()\n",
    "df['Log_std_ProductRelated_Duration'] = np.log1p(df['productRelatedDuration_Standardized'])  # log(1+x)\n",
    "\n",
    "# Summary statistics\n",
    "product_duration_summary = df['Log_std_ProductRelated_Duration'].describe()\n",
    "# Calculate summary statistics\n",
    "product_mean = product_duration_summary.mean()\n",
    "product_median = product_duration_summary.median()\n",
    "product_std_dev = product_duration_summary.std()\n",
    "product_skewness = skew(product_duration_summary)\n",
    "product_kurt = kurtosis(product_duration_summary)\n",
    "\n",
    "print(f\"Mean: {product_mean}\")\n",
    "print(f\"Median: {product_median}\")\n",
    "print(f\"Standard Deviation: {product_std_dev}\")\n",
    "print(f\"Skewness: {product_skewness}\")\n",
    "print(f\"Kurtosis: {product_kurt}\")\n",
    "print(df['ProductRelated_Duration'].describe())\n",
    "print(product_duration_summary)\n",
    "\n",
    "# KDE plot for standardized values\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.kdeplot(product_duration_summary, shade=True)\n",
    "plt.title('Distribution of Standardized Log ProductRelated Duration')\n",
    "plt.xlabel('Standardized Log ProductRelated Duration')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.kdeplot(df['ProductRelated_Duration'], shade=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log and Standardizing Informational Duration (Unfinished)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Standardize the SessionDuration\n",
    "df['infoDuration_Standardized'] = (df['Informational_Duration'] - df['Informational_Duration'].mean()) / df['Informational_Duration'].std()\n",
    "df['Log_std_Informational_Duration'] = np.log1p(df['infoDuration_Standardized'])  # log(1+x)\n",
    "\n",
    "print(df['Informational_Duration'].describe())\n",
    "informational_duration_summary = df['Log_std_Informational_Duration'].describe()\n",
    "print(informational_duration_summary)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.kdeplot(informational_duration_summary, shade=True)\n",
    "plt.title('Distribution of Standardized Log informational Duration')\n",
    "plt.xlabel('Standardized Log informational Duration')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "sns.kdeplot(df['Informational_Duration'], shade=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log and Standardizing Administrative Duration (Unfinished)\n",
    "df['AdminDuration_Standardized'] = (df['Administrative_Duration'] - df['Administrative_Duration'].mean()) / df['Administrative_Duration'].std()\n",
    "df['Log_std_Admin_Duration'] = np.log1p(df['AdminDuration_Standardized'])  # log(1+x)\n",
    "\n",
    "print(df['Administrative_Duration'].describe())\n",
    "Admin_duration_summary = df['Log_std_Admin_Duration'].describe()\n",
    "print(Admin_duration_summary)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.kdeplot(Admin_duration_summary, shade=True)\n",
    "plt.title('Distribution of Standardized Log Admin Duration')\n",
    "plt.xlabel('Standardized Log Admin Duration')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "sns.kdeplot(df['Administrative_Duration'], shade=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering based on Groups \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Assuming you have a dataframe 'df' with 'ProductRelated_Duration' as a column\n",
    "data = df['ProductRelated_Duration'].values\n",
    "\n",
    "# Step 1: Kernel Density Estimation (KDE)\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=500).fit(data[:, np.newaxis])\n",
    "x_d = np.linspace(min(data), max(data), 1000)\n",
    "log_dens = kde.score_samples(x_d[:, np.newaxis])\n",
    "\n",
    "# Plot KDE to visualize peaks\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.fill(x_d, np.exp(log_dens), alpha=0.5)\n",
    "plt.title('Kernel Density Estimation for ProductRelated Duration')\n",
    "plt.xlabel('ProductRelated_Duration')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "# Step 2: Identify the Peaks\n",
    "peaks, _ = find_peaks(np.exp(log_dens), distance=200)\n",
    "peak_values = x_d[peaks]\n",
    "\n",
    "# Sort by peak height (density values) to get the highest peaks\n",
    "sorted_peaks = sorted(peak_values, key=lambda x: np.exp(log_dens)[np.where(x_d == x)[0][0]], reverse=True)\n",
    "\n",
    "first_peak = sorted_peaks[0]  # Highest peak\n",
    "second_peak = sorted_peaks[1]  # Second highest peak\n",
    "\n",
    "# Step 3: Group user sessions based on proximity to the nearest peak\n",
    "def assign_peak_group(duration, first_peak, second_peak):\n",
    "    # Assign to first peak or second peak based on proximity\n",
    "    if abs(duration - first_peak) < abs(duration - second_peak):\n",
    "        return 'First Peak'\n",
    "    else:\n",
    "        return 'Second Peak'\n",
    "\n",
    "df['Session_Group'] = df['ProductRelated_Duration'].apply(lambda x: assign_peak_group(x, first_peak, second_peak))\n",
    "\n",
    "# View the classification results\n",
    "print(df[['ProductRelated_Duration', 'Session_Group']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "#Multi-Grouped Categories \n",
    "second_peak_values = df[df['Session_Group'] == 'Second Peak']\n",
    "first_peak_values = df[df['Session_Group'] == 'First Peak']\n",
    "\n",
    "# Create a summary table for the counts of first and second peak values\n",
    "summary_table = pd.DataFrame({\n",
    "    'Group': ['First Peak', 'Second Peak'],\n",
    "    'Count': [first_peak_values.shape[0], second_peak_values.shape[0]]\n",
    "})\n",
    "print(summary_table)\n",
    "print(summary_table.shape)\n",
    "print(first_peak_values.shape)\n",
    "\n",
    "# Calculate the percentage of values that have revenue in each of the first peak values and second peak values\n",
    "group_first_peak_revenue_percentage = (first_peak_values[first_peak_values['Revenue'] == 1].shape[0] / first_peak_values.shape[0]) * 100\n",
    "group_second_peak_revenue_percentage = (second_peak_values[second_peak_values['Revenue'] == 1].shape[0] / second_peak_values.shape[0]) * 100\n",
    "\n",
    "print(f\"First Peak Revenue Percentage: {group_first_peak_revenue_percentage:.2f}%\")\n",
    "print(f\"Second Peak Revenue Percentage: {group_second_peak_revenue_percentage:.2f}%\")\n",
    "\n",
    "#medianOfDf=df['ProductRelated_Duration'].median()\n",
    "#print(medianOfDf)\n",
    "#print(df[df['ProductRelated_Duration']>medianOfDf].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "#Disregard this grouping\n",
    "# Display the unique session categories\n",
    "print(df['Session_Category'].unique())\n",
    "plt.show()\n",
    "# Count the number of values in each category for 'Session_Category'\n",
    "category_counts = df['Session_Category'].value_counts()\n",
    "print(category_counts)\n",
    "\n",
    "first_peak_values=df[df['Session_Category']=='First Peak']\n",
    "second_peak_values=df[df['Session_Category']=='Second Peak']\n",
    "\n",
    "print(first_peak_values.count())\n",
    "\n",
    "category_first_peak_revenue_percentage = (first_peak_values[first_peak_values['Revenue'] == 1].shape[0] / first_peak_values.shape[0]) * 100\n",
    "category_second_peak_revenue_percentage = (second_peak_values[second_peak_values['Revenue'] == 1].shape[0] / second_peak_values.shape[0]) * 100\n",
    "print(f\"First Peak Revenue Percentage: {category_first_peak_revenue_percentage:.2f}%\")\n",
    "print(f\"Second Peak Revenue Percentage: {category_second_peak_revenue_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Define the preprocessing for numeric and non-numeric features\n",
    "#Visualize the clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='ProductRelated_Duration', y='SessionDuration', hue='Cluster', data=df, palette='viridis')\n",
    "plt.title('Clusters of Sessions based on ProductRelated Duration and Session Duration')\n",
    "plt.xlabel('ProductRelated Duration')\n",
    "plt.ylabel('Session Duration')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n",
    "numeric_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "non_numeric_features = df.select_dtypes(include=['object', 'category', 'bool']).columns\n",
    "\n",
    "# Create a column transformer to apply the appropriate transformations\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), non_numeric_features)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline that first transforms the data and then applies KMeans\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('kmeans', KMeans(n_clusters=3, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipeline.fit(df)\n",
    "\n",
    "# Predict the cluster for each session\n",
    "df['Cluster'] = pipeline.predict(df)\n",
    "\n",
    "#View the first few rows of the dataframe with the cluster assignments\n",
    "#print(df.head())\n",
    "cluster1 = df[df['Cluster']==0]\n",
    "cluster2 = df[df['Cluster']==1]\n",
    "cluster3 = df[df['Cluster']==2]\n",
    "\n",
    "# Create a summary table for the counts of first and second peak values\n",
    "cluster_summary_table = pd.DataFrame({\n",
    "    'Group': ['cluster1','cluster2','cluster3'],\n",
    "    'Count': [cluster1.shape[0], cluster2.shape[0], cluster3.shape[0]]\n",
    "})\n",
    "print(cluster_summary_table)\n",
    "\n",
    "cluster1_revenue_percentage = (cluster1[cluster1['Revenue'] == 1].shape[0] / cluster1.shape[0])\n",
    "cluster2_revenue_percentage = (cluster2[cluster2['Revenue'] == 1].shape[0] / cluster2.shape[0])\n",
    "cluster3_revenue_percentage = (cluster3[cluster3['Revenue'] == 1].shape[0] / cluster3.shape[0])\n",
    "\n",
    "print(df[df['Revenue'] == 1].shape[0] / df['Revenue'].shape[0])\n",
    "print(f\"Number of sessions with Revenue in entire dataset: {df[df['Revenue'] == 1].shape[0]:.2f}\")\n",
    "print(f\"Clustering Comprises of {cluster1_revenue_percentage * cluster1.shape[0]+cluster2_revenue_percentage * cluster2.shape[0]+cluster3_revenue_percentage * cluster3.shape[0]:.2f}\")\n",
    "\n",
    "print(f\"cluster1 Revenue Percentage: {cluster1_revenue_percentage * 100:.2f}% with {cluster1_revenue_percentage * cluster1.shape[0]:.2f} sessions with rev. values\")\n",
    "print(f\"cluster1 Revenue Percentage: {cluster2_revenue_percentage * 100:.2f}% with {cluster2_revenue_percentage * cluster2.shape[0]:.2f} sessions with rev. values\")\n",
    "print(f\"cluster1 Revenue Percentage: {cluster3_revenue_percentage * 100:.2f}% with {cluster3_revenue_percentage * cluster3.shape[0]:.2f} sessions with rev. values\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
